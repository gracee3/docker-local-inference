# vLLM Qwen Server Configuration
# Copy to .env and adjust as needed

# Model path on host
MODEL_PATH=/data/models/Qwen2.5-14B-Instruct-AWQ

# Cache directory (HuggingFace, transformers, torch compile cache)
# Use ~/.cache/vllm or ensure /srv/vllm-cache has proper permissions
CACHE_PATH=~/.cache/vllm

# vLLM tuning (RTX 5000 16GB + 96GB RAM)
# Stable (default): GPU_MEM_UTIL=0.85, MAX_MODEL_LEN=4096
# Note: 8192 context OOMs during CUDA graph warmup on 16GB
GPU_MEM_UTIL=0.85
MAX_MODEL_LEN=4096

# Server port
PORT=8000
