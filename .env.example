# Local LLM / Embedding Server Configuration
# Copy to .env and adjust as needed

# ── GPU pinning ─────────────────────────────────────────
# GPU=0, GPU=1, or GPU=all (default)
GPU=all

# ── Model path (or use PRESET in Makefile) ──────────────
MODEL_PATH=/data/models/Qwen2.5-14B-Instruct-AWQ

# ── Cache directory ─────────────────────────────────────
CACHE_PATH=~/.cache/vllm

# ── vLLM settings ──────────────────────────────────────
GPU_MEM_UTIL=0.85
MAX_MODEL_LEN=8192
PORT=8000

# ── llama.cpp settings ─────────────────────────────────
LLAMA_PORT=8001
LLAMA_N_GPU_LAYERS=999
LLAMA_CTX_SIZE=8192
LLAMA_POOLING=last

# ── Docker Compose overrides ───────────────────────────
# LLM_MODEL_PATH=/data/models/Qwen2.5-14B-Instruct-AWQ
# LLM_PORT=8000
# EMBED_MODEL_PATH=/data/models/nomic-embed-code-Q6_K
# EMBED_PORT=8001
